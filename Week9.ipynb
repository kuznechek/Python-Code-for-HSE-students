{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPWrcMzLmi0oS5S/zV+vzFl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuznechek/Python-Code-for-HSE-students/blob/main/Week9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzR3jo0_L5UZ"
      },
      "source": [
        "Задача 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PieQPnbLzyN"
      },
      "source": [
        "import requests\r\n",
        "\r\n",
        "url = input()\r\n",
        "f = requests.get(url)\r\n",
        "if f.status_code == 200:\r\n",
        "  print('IT WORKS!')\r\n",
        "else:\r\n",
        "  print('ERROR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhz1C0krbEiu"
      },
      "source": [
        "Задача 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAE8qKTLRwQi"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "url = input()\r\n",
        "while url != 'конец':\r\n",
        "  f = requests.get(url)\r\n",
        "  if f.status_code == 200:\r\n",
        "    f.encoding = 'UTF-8'\r\n",
        "    soup = BeautifulSoup(f.text, 'lxml')\r\n",
        "    title = soup.find('title').text\r\n",
        "    print('<a href=\"'+url+'\">'+title+'</a>')\r\n",
        "  url = input()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GYSJN5EbHco"
      },
      "source": [
        "Задача 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi6Wk-zgbOyU"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "url = input()\r\n",
        "f = requests.get(url)\r\n",
        "if f.status_code == 200:\r\n",
        "  f.encoding = 'UTF-8'\r\n",
        "  soup = BeautifulSoup(f.text, 'lxml')\r\n",
        "  files = soup.find_all('img')\r\n",
        "if files == []:\r\n",
        "  print('NO PICS FOUND')\r\n",
        "else:\r\n",
        "  for k in files:\r\n",
        "    print(k.get('src'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_8DpIG9fQCc"
      },
      "source": [
        "Задача 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VzPyuTxyfR67"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "url = input()\r\n",
        "f = requests.get(url)\r\n",
        "if f.status_code == 200:\r\n",
        "  f.encoding = 'UTF-8'\r\n",
        "  soup = BeautifulSoup(f.text, 'lxml')\r\n",
        "  files = soup.find_all('td')\r\n",
        "max, prof = int(files[1].text), files[0].text\r\n",
        "for i in range(2, len(files)-1, 2):\r\n",
        "  if int(files[i+1].text) >  max:\r\n",
        "    max, prof = int(files[i+1].text), files[i].text\r\n",
        "\r\n",
        "print(prof + ': ' + str(max) + ' руб.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLH5yFxohb_j"
      },
      "source": [
        "Задача 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z029r9s_hdnr"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "url = input()\r\n",
        "f = requests.get(url)\r\n",
        "f.encoding = 'UTF-8'\r\n",
        "soup = BeautifulSoup(f.text, 'lxml')\r\n",
        "files = soup.find_all('td')\r\n",
        "d, j = dict(), 0\r\n",
        "for i in range(0, len(files)-1, 2):\r\n",
        "  d[int(files[i+1].text)] = files[i].text\r\n",
        "for k in sorted(d, reverse = True):\r\n",
        "  print(d[k])\r\n",
        "  j += 1\r\n",
        "  if j == 3:\r\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uy5RFdyLR2X"
      },
      "source": [
        "Задача 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzxiSIM8LQos"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "d = dict()\r\n",
        "base_url = input()\r\n",
        "f = requests.get(base_url)\r\n",
        "f.encoding = 'UTF-8'\r\n",
        "soup = BeautifulSoup(f.text, 'lxml')\r\n",
        "urls = soup.find_all('a')\r\n",
        "for i in range(len(urls)):\r\n",
        "  url = base_url + urls[i].get('href')\r\n",
        "  g = requests.get(url)\r\n",
        "  g.encoding = 'UTF-8'\r\n",
        "  minisoup = BeautifulSoup(g.text, 'lxml')\r\n",
        "  d[minisoup.find('i').text] = url\r\n",
        "b = sorted(d)\r\n",
        "print(d[b[0]] + ': ' + b[0] + ' руб.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RqtwzF2zLKu"
      },
      "source": [
        "Задача 7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkStw6mhuJaf"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "phrase = input().split()\r\n",
        "code = input().split(',')\r\n",
        "url = input()\r\n",
        "f = requests.get(url)\r\n",
        "f.encoding = 'UTF-8'\r\n",
        "soup = BeautifulSoup(f.text, 'lxml')\r\n",
        "urls = soup.find_all('td') + soup.find_all('th')\r\n",
        "d, s = dict(), list()\r\n",
        "if code[0] == 'ENG':\r\n",
        "  for k in range(0, len(urls)-1, 2):\r\n",
        "    d[urls[k].text] = urls[k+1].text \r\n",
        "elif code[0] == 'RUS':\r\n",
        "  for k in range(1, len(urls), 2):\r\n",
        "    d[urls[k].text] = urls[k-1].text\r\n",
        "for word in phrase:\r\n",
        "  s.append(d[word])\r\n",
        "print(\" \".join(\" \".join(s).split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyYSM7EjzNJM"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "phrase = input().lower().split()\r\n",
        "code = 'ENG,RUS'.split(',')\r\n",
        "url = 'https://online.hse.ru/python-as-foreign/tasks/dict/example.html'\r\n",
        "f = requests.get(url)\r\n",
        "f.encoding = 'UTF-8'\r\n",
        "soup = BeautifulSoup(f.text, 'lxml')\r\n",
        "urls = soup.find_all('td') + soup.find_all('th')\r\n",
        "d, s = dict(), list()\r\n",
        "if code[0] == 'ENG':\r\n",
        "  for k in range(0, len(urls)-1, 2):\r\n",
        "    d[urls[k].text] = urls[k+1].text \r\n",
        "elif code[0] == 'RUS':\r\n",
        "  for k in range(1, len(urls), 2):\r\n",
        "    d[urls[k].text] = urls[k-1].text\r\n",
        "print(d)\r\n",
        "for word in phrase:\r\n",
        "  s.append(d[word])\r\n",
        "print(\" \".join(\" \".join(s).split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AI4XDXnc78Os"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "def translator(sentence, words, encode):\r\n",
        "  s = list()\r\n",
        "  if encode == 0:\r\n",
        "    for word in sentence:\r\n",
        "      if word in words:\r\n",
        "        i = words.index(word)\r\n",
        "        s.append(words[i+1])\r\n",
        "  elif encode == 1:\r\n",
        "    for word in sentence:\r\n",
        "      if word in words:\r\n",
        "        i = words.index(word)\r\n",
        "        s.append(words[i-1])\r\n",
        "  return s\r\n",
        "\r\n",
        "phrase = input().split()\r\n",
        "code = input().split(',')\r\n",
        "url = input()\r\n",
        "f = requests.get(url)\r\n",
        "f.encoding = 'UTF-8'\r\n",
        "soup = BeautifulSoup(f.text, 'lxml')\r\n",
        "urls = soup.find_all('td')\r\n",
        "for k in range(len(urls)):\r\n",
        "  urls[k] = urls[k].text\r\n",
        "if code[0] == 'ENG' and code[1] == 'RUS':\r\n",
        "  j = translator(phrase, urls, 0)\r\n",
        "elif code[0] == 'RUS' and code[1] == 'ENG':\r\n",
        "  j = translator(phrase, urls, 1)\r\n",
        "if j != []:\r\n",
        "  print(\" \".join(\" \".join(j).split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA5JgQNKGoF4"
      },
      "source": [
        "Задача 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEkqQssbGqG5"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "def make_set(site):\r\n",
        "  s = set()\r\n",
        "  f = requests.get(site)\r\n",
        "  f.encoding = 'UTF-8'\r\n",
        "  soup = BeautifulSoup(f.text, 'lxml')\r\n",
        "  urls = soup.find_all('td')\r\n",
        "  for k in range(0, len(urls)-4, 5):\r\n",
        "    s.add(urls[k].text)\r\n",
        "  return s\r\n",
        "\r\n",
        "m = set()\r\n",
        "url = input()\r\n",
        "if url != 'конец':\r\n",
        "  n = make_set(url)\r\n",
        "  url = input()\r\n",
        "  while url != 'конец':\r\n",
        "    m = make_set(url)\r\n",
        "    n = n.intersection(m)\r\n",
        "    url = input()\r\n",
        "for k in sorted(n):\r\n",
        "  print(k)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9PLxdwrZnVI"
      },
      "source": [
        "Задача 9"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iskv2O0TZo7L"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "def add_to_dict(site, d):\r\n",
        "  f = requests.get(site)\r\n",
        "  f.encoding = 'UTF-8'\r\n",
        "  soup = BeautifulSoup(f.text, 'lxml')\r\n",
        "  urls = soup.find_all('td')\r\n",
        "  for i in range(0, len(urls)-4, 5):\r\n",
        "    key = urls[i+1].text\r\n",
        "    if key not in d: \r\n",
        "      d[key] = dict()\r\n",
        "    if key in d:\r\n",
        "      rate = urls[i+3].text\r\n",
        "      if rate not in d[key]:\r\n",
        "        d[key][rate] = set()\r\n",
        "      if rate in d[key]:\r\n",
        "        d[key][rate].add(urls[i].text + ' (' + urls[i+4].text + ')')\r\n",
        "\r\n",
        "url = input()\r\n",
        "dic = dict()\r\n",
        "while 'http' in url:\r\n",
        "  add_to_dict(url, dic)\r\n",
        "  url = input()\r\n",
        "director = url\r\n",
        "if director in  dic:\r\n",
        "  for mark in sorted(dic[director], reverse = True):\r\n",
        "    for j in sorted(dic[director][mark]):\r\n",
        "      print(j)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xpv-jXWuhCXF"
      },
      "source": [
        "Задача 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r94yg7gAhDh0",
        "outputId": "69b0c057-1f5d-453f-aff7-9bd2b061f6f1"
      },
      "source": [
        "import requests\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "def get_soup(cur_url):\r\n",
        "  f = requests.get(cur_url)\r\n",
        "  f.encoding = 'UTF-8'\r\n",
        "  soup = BeautifulSoup(f.text, 'lxml')\r\n",
        "  return soup\r\n",
        "\r\n",
        "def make_textlist(listik):\r\n",
        "  for i in range(len(listik)):\r\n",
        "    listik[i] = listik[i].text\r\n",
        "  return listik\r\n",
        "\r\n",
        "base_url = input()\r\n",
        "man = input().upper()\r\n",
        "columns = input().split(',')\r\n",
        "urls = set()\r\n",
        "url = base_url\r\n",
        "while True:\r\n",
        "  soupchik = get_soup(url)\r\n",
        "  urls.add(url)\r\n",
        "  next_link = soupchik.find('a', text = 'Следующая таблица результатов')\r\n",
        "  if not next_link:   \r\n",
        "    break\r\n",
        "  next_url = next_link.get('href')\r\n",
        "  url = base_url + next_url\r\n",
        "urls = sorted(urls)\r\n",
        "strings = make_textlist(soupchik.find_all('th'))\r\n",
        "for i in range(len(columns)):\r\n",
        "  columns[i] = int(strings.index(columns[i]))\r\n",
        "for url in urls:\r\n",
        "  soupchik = get_soup(url)\r\n",
        "  hole = make_textlist(soupchik.find_all('td'))\r\n",
        "  ans = list()\r\n",
        "  for w in hole:\r\n",
        "    if man in w:\r\n",
        "      ind = int(hole.index(w))\r\n",
        "      for c in columns:\r\n",
        "        ans.append(hole[ind + c - 1])\r\n",
        "      print(\" \".join(ans))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://online.hse.ru/python-as-foreign/tasks/fs/example/\n",
            "плисецкий\n",
            "TSS\n",
            "82.27\n",
            "59.29\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}